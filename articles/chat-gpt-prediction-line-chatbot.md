---
title: "ChatGPT ã§ Bing Chat ã‚¹ã‚¿ã‚¤ãƒ«ã®è³ªå•ææ¡ˆã‚’ã™ã‚‹ LINE ãƒœãƒƒãƒˆã‚’ä½œã‚‹"
emoji: "ğŸ¤–"
type: "tech" # tech: æŠ€è¡“è¨˜äº‹ / idea: ã‚¢ã‚¤ãƒ‡ã‚¢
topics:
  - "ChatGPT"
  - "OpenAI"
  - "LINE"
  - "AWS"
  - "Amplify"
published: true
---

ã“ã‚“ã«ã¡ã‚ã€‚ [ZUMA](https://twitter.com/zuma_lab) ã§ã™ã€‚

ChatGPT ã§ Bing Chat ã®ã‚ˆã†ã«ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•äºˆæ¸¬ã‚’ã™ã‚‹ LINE ãƒœãƒƒãƒˆ ã‚’ä½œã£ã¦ã¿ã¾ã—ãŸã€‚

ChatGPT ã®å›ç­”ã‚’å…ƒã« ChatGPT ãŒãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•äºˆæ¸¬ã‚’ã™ã‚‹ã®ã§ã€è„³æ­»ã§ ChatGPT ã«ç„¡é™è³ªå•ã™ã‚‹äº‹ãŒã§ãã¾ã™ã€‚

# æˆæœç‰©

ä»¥ä¸‹æˆæœç‰©ã¨ãªã‚Šã¾ã™ã€‚

https://youtu.be/rS7qfRgu4f0

LINE ã®ç”»é¢ä¸‹éƒ¨ã«ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒæ¬¡ã«ã™ã‚‹è³ªå•äºˆæ¸¬ã‚’ãƒœã‚¿ãƒ³ã§è¡¨ç¤ºã—ã¦ã„ã¾ã™ã€‚

ãƒœã‚¿ãƒ³ã‚’ã‚¿ãƒƒãƒ—ã™ã‚‹ã¨ãã®è³ªå•ã‚’ ChatGPT ã«èãã“ã¨ãŒã§ãã¾ã™ã€‚

(ãƒ‡ãƒ¢ã¯ GPT-4 API ã‚’åˆ©ç”¨ã—ã¦ã„ã‚‹ã®ã§ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãŒé…ã„ã§ã™)

ã¾ãŸã€ã“ã®ã‚ˆã†ã«ä¼šè©±å±¥æ­´ã¨æ–‡è„ˆã‚’èª­ã‚“ã§å›ç­”ã‚’ã—ã¦ãã‚Œã¾ã™ã€‚

ã¡ãªã¿ã« ChatGPT ã¯ãƒ•ãƒ¬ãƒ³ãƒ‰ãƒªãƒ¼ã§çµµæ–‡å­—ã‚’ã‚„ãŸã‚‰ä½¿ã†ã‚­ãƒ£ãƒ©è¨­å®šã‚’ã—ã¦ã‚‹ã®ã§å°‘ã—ç”»é¢ãŒã†ã‚‹ã•ã„ã®ã¯ã”äº†æ‰¿ãã ã•ã„ã€‚

![](https://storage.googleapis.com/zenn-user-upload/40bc779847c9-20230414.jpg =400x)
![](https://storage.googleapis.com/zenn-user-upload/63bc290236d9-20230414.jpg =400x)

ç”»é¢ä¸‹éƒ¨ã«è¡¨ç¤ºã•ã‚Œã¦ã„ã‚‹ã®ãŒè³ªå•äºˆæ¸¬ãƒœã‚¿ãƒ³ã§ã™ã€‚

ChatGPT ã®å›ç­”ã‚’å…ƒã« ChatGPT ãŒãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã‚’äºˆæ¸¬ã—ãŸçµæœã‚’è¡¨ç¤ºã—ã¦ã„ã¾ã™ã€‚

# å‰æ

å‰æã¨ã—ã¦æœ¬è¨˜äº‹ã¯ä»¥ä¸‹ã® LINE ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆä½œæˆè¨˜äº‹ã‚’ãƒ™ãƒ¼ã‚¹ã«ã—ã¦ã„ã¾ã™ã€‚

https://zenn.dev/zuma_lab/articles/gpt-4-line-chatbot

ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ç’°å¢ƒæ§‹ç¯‰ã¯[Amplify CLI ã§ãƒªã‚½ãƒ¼ã‚¹ã‚’ä½œæˆã™ã‚‹](https://zenn.dev/zuma_lab/articles/gpt-4-line-chatbot#amplify-cli-%E3%81%A7%E3%83%AA%E3%82%BD%E3%83%BC%E3%82%B9%E3%82%92%E4%BD%9C%E6%88%90%E3%81%99%E3%82%8B)ä»¥é™ã‚’å‚ç…§ãã ã•ã„ã€‚

Amplify ã®ç’°å¢ƒæ§‹ç¯‰ã€REST API ã‹ã‚‰ DynamoDB ã®ä½œæˆæ–¹æ³•ã€ LINE è¨­å®šã€OpenAI ã® API ã‚­ãƒ¼å–å¾—æ–¹æ³•ã‚’è§£èª¬ã—ã¦ã„ã¾ã™ã€‚

ã¾ãŸã€DynamoDB ã‚’ç”¨ã„ã¦ä¼šè©±ã‚„æ–‡è„ˆã‚’èª­ã‚“ã å®Ÿè£…ã‚’ã—ã¦ã„ã¾ã™ã€‚

ã¾ãšã¯ä¸Šè¨˜æ‰‹é †ã§ LINE ãƒœãƒƒãƒˆã®ãƒ™ãƒ¼ã‚¹ä½œæˆå¾Œã€ã‚³ãƒ¼ãƒ‰ã‚’å®Ÿè£…ã—ã¦ãã ã•ã„ã€‚

# ã©ã†ã‚„ã£ã¦ã‚‹ã®ï¼Ÿ

æœ€åˆã«ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’ï¼’ã¤è€ƒãˆã¾ã—ãŸã€‚

- ChatGPT ã®å›ç­”ã¨è³ªå•äºˆæ¸¬ã‚’ã‚»ãƒƒãƒˆã§å‡ºåŠ›ã™ã‚‹ã‚ˆã†æŒ‡ç¤º
- ChatGPT ã®å›ç­”ã‚’å…ƒã« LangChain ã§è³ªå•äºˆæ¸¬ã‚’å–å¾—

LangChain ã®èª¬æ˜ã¯çœãã¾ã™ãŒã€ LLM ã‚’ä½¿ã£ã¦ã‚µãƒ¼ãƒ“ã‚¹ã‚’é–‹ç™ºã—ãŸã„ã¨ãã®ã‚ˆãã‚ã‚‹æ©Ÿèƒ½ã‚’ã¾ã¨ã‚ã¦æä¾›ã—ã¦ãã‚Œã¦ã„ã‚‹ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã§ã™ã€‚

2 ãƒ‘ã‚¿ãƒ¼ãƒ³ãã‚Œãã‚Œ Pros/Cons ã‚’è€ƒãˆã¾ã—ãŸã€‚

| æ–¹æ³•                         | Pros                       | Cons                                                   |
| ---------------------------- | -------------------------- | ------------------------------------------------------ |
| å›ç­”ã¨è³ªå•äºˆæ¸¬ã‚’ã‚»ãƒƒãƒˆã§å‡ºåŠ› | ä¸€åº¦ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆã§æ¸ˆã‚€     | è³ªå•äºˆæ¸¬ã‚’å‡ºåŠ›ã—ãªã„å ´åˆãŒã‚ã‚‹ã€‚æ¶ˆè²»ãƒˆãƒ¼ã‚¯ãƒ³æ•°ãŒå¤šã„ã€‚ |
| LangChain ã§åˆ¥ã€…ã«å‡ºåŠ›       | è³ªå•äºˆæ¸¬ãŒç¢ºå®Ÿã«å–å¾—ã§ãã‚‹ | 2 å›ãƒªã‚¯ã‚¨ã‚¹ãƒˆãŒé€ä¿¡ã•ã‚Œã‚‹ã®ã§å›ç­”ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãŒé…ã„     |

ãã‚Œã§ã¯å„ãƒ‘ã‚¿ãƒ¼ãƒ³ã®è§£èª¬ã§ã™ã€‚

## å›ç­”ã¨è³ªå•äºˆæ¸¬ã‚’ã‚»ãƒƒãƒˆã§å‡ºåŠ›ã™ã‚‹ãƒ‘ã‚¿ãƒ¼ãƒ³

ã‚„ã‚Šæ–¹ã¨ã—ã¦ã¯å˜ç´”ã§ã€ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«ä»¥ä¸‹ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’çµ„ã¿è¾¼ã‚“ã§ã„ã¾ã™ã€‚

```
ãƒ•ã‚©ãƒ¼ãƒãƒ«ãªè¨€è‘‰é£ã„ã¯ã‚„ã‚ã¦ãã ã•ã„ã€‚å‹é”ã®ã‚ˆã†ã«ãƒ•ãƒ¬ãƒ³ãƒ‰ãƒªãƒ¼ãªå£èª¿ã§ã€è©±ã™ã¨ãã«ã¯ãŸãã•ã‚“ã®çµµæ–‡å­—ã‚’ä½¿ã£ã¦ãã ã•ã„ã€‚
ã¾ãŸã€ä»¥ä¸‹ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã«å¾“ã£ã¦å›ç­”ã—ã¦ãã ã•ã„ã€‚

# ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ

ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã¸ã®å›ç­”

### Predictions ###
ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æ¬¡ã®3ã¤ã®è³ªå•ã‚’äºˆæ¸¬ã—ã¦ã€ãã‚Œãã‚Œ20æ–‡å­—ä»¥å†…ã§ãƒªã‚¹ãƒˆã‚¢ãƒƒãƒ—
```

è³ªå•äºˆæ¸¬ã¯ 3 ã¤ã§ 20 æ–‡å­—ä»¥å†…ã§å›ç­”ã™ã‚‹ã‚ˆã†ã«ã—ã¾ã™ã€‚

ãªãœ 20 æ–‡å­—ä»¥å†…ã‹ã¨ã„ã†ã¨ã€LINE ã«è¡¨ç¤ºã§ãã‚‹è³ªå•äºˆæ¸¬ãƒœã‚¿ãƒ³ã®æœ€å¤§æ–‡å­—æ•°ãŒ 20 æ–‡å­—ã ã‹ã‚‰ã§ã™ã€‚

è©¦ã—ã« `æ—¥æœ¬é£Ÿã§å®‰ãé£Ÿäº‹ãŒã§ãã‚‹ã‚¸ãƒ£ãƒ³ãƒ«ã¯ä½•ã§ã™ã‹ï¼Ÿ` ã¨è³ªå•ã—ã¦ã¿ã¾ã™ã€‚

ChatGPT ã‹ã‚‰ã¯ä»¥ä¸‹ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§å›ç­”ãŒè¿”ã£ã¦ãã¾ã™ã€‚

```
å¯¿å¸ã‚„ãƒ©ãƒ¼ãƒ¡ãƒ³ã€å¤©ã·ã‚‰ã€ç„¼ãé³¥ãªã©ã¯ã€æ—¥æœ¬ã®å±…é…’å±‹ã‚„ãƒ¬ã‚¹ãƒˆãƒ©ãƒ³ã§æ¯”è¼ƒçš„æ‰‹è»½ã«é£Ÿã¹ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚

### Predictions ###
1. ã©ã®ãã‚‰ã„ã®å€¤æ®µã§ã™ã‹ï¼Ÿ
2. ãã®ä¸­ã§é«˜ç´šãªãŠåº—ã¯ã©ã“ã§ã™ã‹ï¼Ÿ
3. ãŠã™ã™ã‚ã®ãŠåº—ã¯ã‚ã‚Šã¾ã™ã‹ï¼Ÿ
```

å›ç­”çµæœã® `### Predictions ###` ã®è­˜åˆ¥å­ãŒã‚ã£ãŸã‚‰ä»¥é™ã®æ–‡å­—åˆ—ã‚’è³ªå•äºˆæ¸¬ã¨è¦‹ãªã—ã¦ãƒœã‚¿ãƒ³ã«ã‚»ãƒƒãƒˆã—ã¾ã™ã€‚

ä»¥ä¸‹ ãƒ—ãƒ­ãƒ€ã‚¯ãƒˆã«çµ„ã¿è¾¼ã‚€ Python ã‚³ãƒ¼ãƒ‰å…¨æ–‡ã¨ãªã‚Šã¾ã™ã€‚

:::details chatgpt_api.py

```py
import const
import openai
import re
from typing import List, Tuple, Dict

# Model name
GPT_MODEL = 'gpt-4'  # or gpt-3.5-turbo

# Temperature
TEMPERATURE = 0.7

PREDICTION_KEYWORD = '### Predictions ###'

SYSTEM_PROMPT = f'''ãƒ•ã‚©ãƒ¼ãƒãƒ«ãªè¨€è‘‰é£ã„ã¯ã‚„ã‚ã¦ãã ã•ã„ã€‚å‹é”ã®ã‚ˆã†ã«ãƒ•ãƒ¬ãƒ³ãƒ‰ãƒªãƒ¼ãªå£èª¿ã§ã€è©±ã™ã¨ãã«ã¯ãŸãã•ã‚“ã®çµµæ–‡å­—ã‚’ä½¿ã£ã¦ãã ã•ã„ã€‚
ã¾ãŸã€ä»¥ä¸‹ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã«å¾“ã£ã¦å›ç­”ã—ã¦ãã ã•ã„ã€‚

# ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ

ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã¸ã®å›ç­”

{PREDICTION_KEYWORD}
ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æ¬¡ã®3ã¤ã®è³ªå•ã‚’äºˆæ¸¬ã—ã¦ã€ãã‚Œãã‚Œ20æ–‡å­—ä»¥å†…ã§ãƒªã‚¹ãƒˆã‚¢ãƒƒãƒ—
'''

DEFAULT_PREDICTION_TEXT = f'''{PREDICTION_KEYWORD}
1. ChatGPTã£ã¦ä½•ã§ã™ã‹ï¼Ÿ
2. ChatGPTã§ã¯ä½•ãŒã§ãã¾ã™ã‹ï¼Ÿ
3. ã©ã‚“ãªè³ªå•ã«ç­”ãˆã¦ãã‚Œã¾ã™ã‹ï¼Ÿ
'''

SYSTEM_PROMPTS = [{'role': 'system', 'content': SYSTEM_PROMPT}]


def _parse_completed_text(completed_text: str) -> Tuple[str, List[str]]:
    # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§åˆ†å‰²ã€‚ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã¯å«ã¾ãªã„ã€‚ç¬¬2å¼•æ•°ã¯åˆ†å‰²æ•°ã€‚
    completed_texts = completed_text.split(PREDICTION_KEYWORD, 1)
    if len(completed_texts) < 2:
        raise Exception('The keyword is not found in the text.')
    # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«è¿”å´ã™ã‚‹ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—ã€‚æ–‡å­—åˆ—ã®å‰å¾Œã®ç©ºç™½ã¨æ”¹è¡Œã‚’å‰Šé™¤
    assistant_answer = completed_texts[0].strip()
    # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ä»¥é™ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—
    prediction_text = completed_texts[1]
    # æ”¹è¡Œã§åˆ†å‰²ã—ã€æ–‡å­—åˆ—ã®å‰å¾Œã®ç©ºç™½ã¨æ”¹è¡Œã‚’å‰Šé™¤
    predictions = list(map(lambda line: _remove_ordinal_number(line.strip()), prediction_text.strip().split('\n')))
    return assistant_answer, predictions


# æ–‡å­—åˆ—ã®å…ˆé ­ã«ã‚ã‚‹åºæ•°ã‚’å‰Šé™¤ã™ã‚‹é–¢æ•°
def _remove_ordinal_number(text: str) -> str:
    # æ­£è¦è¡¨ç¾ã§å…ˆé ­ã®æ•°å­—ã¨ãƒ”ãƒªã‚ªãƒ‰ã‚’å‰Šé™¤
    return re.sub(r'^\d+\.\s*', '', text)


def _print_total_length(completed_text, messages):
    join_message = completed_text + ' ' + ' '.join(map(lambda message: message['content'], messages))
    print(completed_text.replace('\n', ''))
    print(f"total length:{len(join_message)}")


def completions(history_prompts: List[Dict[str, str]]) -> Tuple[str, str, List[str]]:
    messages = SYSTEM_PROMPTS + history_prompts

    try:
        openai.api_key = const.OPEN_AI_API_KEY
        response = openai.ChatCompletion.create(
            model=GPT_MODEL,
            messages=messages,
            temperature=TEMPERATURE,
        )
        completed_text = response['choices'][0]['message']['content']

        if PREDICTION_KEYWORD not in completed_text:
            completed_text = f'{completed_text}\n{DEFAULT_PREDICTION_TEXT}'

        _print_total_length(completed_text, messages)

        assistant_answer, predictions = _parse_completed_text(completed_text)

        return completed_text, assistant_answer, predictions
    except Exception as e:
        raise e
```

:::

### Pros

ãƒ¡ãƒªãƒƒãƒˆã¯ä¸€åº¦ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆã§å›ç­”ã¨è³ªå•äºˆæ¸¬ãŒå¾—ã‚‰ã‚Œã‚‹ã®ã§ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¸ã®å›ç­”é€Ÿåº¦ãŒä¸ŠãŒã‚Šã¾ã™ã€‚

ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãŒé…ã„ GPT-4 API ã§ã‚‚è³ªå•äºˆæ¸¬ã‚’å–å¾—ã—ãŸã„å ´åˆã¯ã“ã¡ã‚‰ã®ã‚„ã‚Šæ–¹ãŒè‰¯ã•ãã†ã§ã™ã€‚

### Cons

ã“ã®ã‚„ã‚Šæ–¹ã®æ¬ ç‚¹ã¨ã—ã¦ã¯ã€ä»Šå›ã®ã‚ˆã†ã«ä¼šè©±å±¥æ­´ã¨æ–‡è„ˆã‚’èª­ã‚“ã å›ç­”ã‚’ã•ã›ã¦ã„ã‚‹ã‚±ãƒ¼ã‚¹ã§ã¯ã€éå»ã®ä¼šè©±å±¥æ­´ã«ã‚‚å…¨ã¦è³ªå•äºˆæ¸¬ã‚’å«ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚

ChatGPT ã¯éå»ã®ä¼šè©±å±¥æ­´ã§ç„¡ã„æ–‡è„ˆã«ã¤ã„ã¦ã¯ç­”ãˆã¦ãã‚Œã¾ã›ã‚“ã€‚

éå»ä¼šè©±ã« `### Predictions ###` ã®è­˜åˆ¥å­ãŒç„¡ã‘ã‚Œã° `ä»¥ä¸‹ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã«å¾“ã£ã¦å›ç­”ã—ã¦ãã ã•ã„ã€‚` ã¨ã„ã†æŒ‡ç¤ºã¯ç„¡è¦–ã•ã‚Œã¾ã™ã€‚

ã‚ˆã£ã¦ã€Predictions ã®è­˜åˆ¥å­ã¨è³ªå•äºˆæ¸¬ã‚’å…¨ã¦è³ªå•æ¯ã®ä¼šè©±å±¥æ­´ã«å…¥ã‚Œè¾¼ã¾ãªã„ã¨ã„ã‘ãªã„ã®ã§ãƒˆãƒ¼ã‚¯ãƒ³æ•°ãŒå¢—ãˆã¦ã„ãã¾ã™ã€‚

ä¼šè©±å±¥æ­´ã‚’å«ã‚ãªã„ QA ãƒœãƒƒãƒˆã§ã‚ã‚Œã°æ°—ã«ã—ãªãã¦ã„ã„ã§ã™ãŒã€æ–‡è„ˆã‚’èª­ã‚€ãƒœãƒƒãƒˆã®å ´åˆã¯ã‚³ã‚¹ãƒˆãŒé«˜ããªã£ã¦ã„ãã¾ã™ã€‚

ã¡ãªã¿ã«ã€ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®æŒ‡ç¤ºã‚’è‹±èªã«ã—ãŸå ´åˆã‚‚å‹•ä½œãŒä¸å®‰å®šã§ã™ã€‚

ãªãœãªã‚‰å›ç­”ãŒæ—¥æœ¬èªãªã®ã§ã€ `### Predictions ###` ãŒ `### äºˆæ¸¬ ###` ç­‰ç¿»è¨³ã•ã‚Œã¦ã—ã¾ã„ã€å‹•ä½œãŒä¸å®‰å®šã«ãªã‚Šã¾ã™ã€‚

ã¾ãŸã€ä»Šå¾Œç™»å ´ã™ã‚‹ AI ãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã£ã¦ã‚‚æŒ™å‹•ãŒå¤‰ã‚ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚

## LangChain ã§å›ç­”ã¨è³ªå•äºˆæ¸¬ã‚’åˆ¥ã€…ã«å‡ºåŠ›ã™ã‚‹ãƒ‘ã‚¿ãƒ¼ãƒ³

LangChain ã® Memory in Sequential Chains ã‚’åˆ©ç”¨ã—ã¾ã™ã€‚

https://python.langchain.com/en/latest/modules/chains/generic/sequential_chains.html#memory-in-sequential-chains

ã¾ãšéå»ã®ä¼šè©±å±¥æ­´ã®ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ç”¨æ„ã—ã¾ã™ã€‚

æ¬¡ã«å›ç­”ç”¨ LLM ã¨è³ªå•äºˆæ¸¬ç”¨ LLM ã‚’ä½œæˆã—ã¾ã™ã€‚

è³ªå•äºˆæ¸¬ç”¨ LLM ã¯å›ç­”ç”¨ LLM ã®å‡ºåŠ›ã‚’å…¥åŠ›ã¨ã—ã¦è³ªå•äºˆæ¸¬ã—ã¾ã™ã€‚

LangChain ã® Chains ã¨ã„ã†ä»•çµ„ã¿ã‚’ä½¿ã£ã¦ã€å›ç­”ç”¨ LLM -> è³ªå•äºˆæ¸¬ç”¨ LLM ã¨å®Ÿè¡Œé †ç•ªã‚’æ±ºã‚ã¾ã™ã€‚

ä»Šå›ã¯ SequentialChain ã® memory æ©Ÿèƒ½ã‚’ä½¿ã£ã¦ä¼šè©±å±¥æ­´ã‚’èª­ã¿è¾¼ã‚“ã§å›ç­”ç”¨ LLM ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«åŸ‹ã‚è¾¼ã‚“ã§ã„ã¾ã™ã€‚

ä»¥ä¸‹ Google Colab ä¸Šã§å‹•ãã‚³ãƒ¼ãƒ‰ã§ã™ã€‚

```
!pip install langchain
!pip install openai
```

ã¾ãšã€éå»ã®ä¼šè©±å±¥æ­´ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’è¨­å®šã—ã¾ã™ã€‚

ã“ã¡ã‚‰ã¯ Colab ç”¨ã§ä¼šè©±å±¥æ­´ã‚’ç›´æ›¸ãã—ã¦ã¾ã™ãŒã€å®Ÿéš›ã¯ DynamoDB ã‹ã‚‰å–å¾—ã—ã¾ã™ã€‚

AI ã¨ Human ã®éå»ã®ä¼šè©±ã‚’ SimpleMemory ã«è¨­å®šã™ã‚‹äº‹ã«ã‚ˆã‚Šæ–‡è„ˆã‚’èª­ã‚“ã å›ç­”ã‚’ã—ã¾ã™ã€‚

```py
from langchain.memory import SimpleMemory

OPENAI_API_KEY='Your API Key'

GPT_MODEL='gpt-3.5-turbo'
### ä¼šè©±å±¥æ­´
history = """Human: é£Ÿã¹ç‰©ã®ä¸­ã§ä½•ãŒå¥½ãï¼Ÿ
AI: ç§ã¯é£Ÿã¹ç‰©ã‚’é£Ÿã¹ã‚‹ã“ã¨ã¯ã§ãã¾ã›ã‚“ãŒã€æ—¥æœ¬ã®æ–‡åŒ–ã«èˆˆå‘³ã‚’æŒã£ã¦ãŠã‚Šã€å¯¿å¸ã‚„ãƒ©ãƒ¼ãƒ¡ãƒ³ã€å¤©ã·ã‚‰ã€ç„¼ãé³¥ãªã©ã®æ—¥æœ¬æ–™ç†ãŒå¥½ãã§ã™ã€‚ã¾ãŸã€ã‚¤ãƒ³ãƒ‰ã‚«ãƒ¬ãƒ¼ã‚„ã‚¿ã‚¤æ–™ç†ã€ãƒ¡ã‚­ã‚·ã‚³æ–™ç†ãªã©ã€å¤šæ§˜ãªå›½ã®æ–™ç†ã‚‚å¥½ãã§ã™ã€‚
Human: ãã®ä¸­ã§ä½•ãŒå¥½ãï¼Ÿ
AI: ç§ã¯ãŠã™ã™ã‚ã®æ–™ç†ã‚’é¸ã¶ã“ã¨ãŒã§ãã¾ã›ã‚“ãŒã€æ—¥æœ¬ã®å¯¿å¸ã¯éå¸¸ã«äººæ°—ãŒã‚ã‚Šã€æ–°é®®ãªé­šä»‹é¡ã‚’ä½¿ç”¨ã—ã¦ã„ã‚‹ã“ã¨ãŒå¤šã„ãŸã‚ã€ãŠã™ã™ã‚ã§ã™ã€‚ã¾ãŸã€ãƒ©ãƒ¼ãƒ¡ãƒ³ã¯æ—¥æœ¬ã®ã‚½ã‚¦ãƒ«ãƒ•ãƒ¼ãƒ‰ã®ä¸€ã¤ã§ã€æ§˜ã€…ãªç¨®é¡ãŒã‚ã‚Šã¾ã™ãŒã€è±šéª¨ãƒ©ãƒ¼ãƒ¡ãƒ³ã‚„é†¤æ²¹ãƒ©ãƒ¼ãƒ¡ãƒ³ãŒç‰¹ã«äººæ°—ãŒã‚ã‚Šã¾ã™ã€‚å¤©ã·ã‚‰ã‚‚ã€ã‚µã‚¯ã‚µã‚¯ã¨ã—ãŸé£Ÿæ„ŸãŒç¾å‘³ã—ã„ã§ã™ã€‚ç„¼ãé³¥ã¯ã€ä¸²ã«åˆºã—ãŸé¶è‚‰ã‚’ç‚­ç«ã§ç„¼ã„ãŸã‚‚ã®ã§ã€ãƒ“ãƒ¼ãƒ«ã¨ã®ç›¸æ€§ãŒè‰¯ã„ã§ã™ã€‚
"""

memories = {"history": history}
memory = SimpleMemory(memories=memories)
```

æ¬¡ã« ChatGPT ã®å›ç­”ç”¨ LLM ã®ä½œæˆã§ã™ã€‚

question_template ã§ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã®æ€§æ ¼ã€ä¼šè©±å±¥æ­´ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½œæˆã—ã¾ã™ã€‚

ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ç²¾åº¦ã‚’é«˜ã‚ã‚‹ç‚ºã«è‹±èªã§è³ªå•ã—ã¦æœ€å¾Œã« `answer in Japanese language` ã¨æ—¥æœ¬èªã§å‡ºåŠ›ã•ã›ã¾ã™ã€‚

ChatOpenAI ã® temperature ã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã® 0.7 ã‚’è¨­å®šã—ã¦ã„ã¾ã™ãŒã€ã“ã“ã¯ãŠå¥½ã¿ã§å¤‰æ›´ã—ã¦ãã ã•ã„ã€‚

```py
from langchain import PromptTemplate, LLMChain
from langchain.chat_models import ChatOpenAI

### è³ªå•
# question_templateè¨³
# ä»¥ä¸‹ã¯ã€äººé–“ã¨AIã®å‹å¥½çš„ãªä¼šè©±ã§ã™ã€‚
# AIã¯ãŠã—ã‚ƒã¹ã‚Šã§ã€ãã®æ–‡è„ˆã‹ã‚‰ãŸãã•ã‚“ã®å…·ä½“çš„ãªè©³ç´°ã‚’æä¾›ã—ã¾ã™ã€‚
# ãã‚Œã‹ã‚‰ã€ãƒ•ã‚©ãƒ¼ãƒãƒ«ãªè¨€è‘‰ã‚’ä½¿ã†ã®ã‚’ã‚„ã‚ã¦ã€å‹é”ã®ã‚ˆã†ã«è¦ªã—ã¿ã‚„ã™ãè©±ã—ã¦ãã ã•ã„ã€‚ã¾ãŸã€ãŸãã•ã‚“ã®çµµæ–‡å­—ã‚’ä½¿ã£ã¦è©±ã—ã¦ãã ã•ã„ã€‚
# AIãŒè³ªå•ã«ç­”ãˆã‚‰ã‚Œãªã„å ´åˆã¯ã€æ­£ç›´ã«ã‚ã‹ã‚‰ãªã„ã¨è¨€ã„ã¾ã™ã€‚
# æ—¥æœ¬èªã§ãŠç­”ãˆãã ã•ã„ã€‚

question_template = """The following is a friendly conversation between a human and an AI.
The AI is talkative and provides lots of specific details from its context.
After that, stop using formal language. Talk to me in a friendly way, like a friend. Also, use lots of emojis when you talk.
If the AI does not know the answer to a question, it truthfully says it does not know.
Please answer in Japanese language.

Current conversation:
{history}
Human: {question}
AI:"""

question_prompt_template = PromptTemplate(input_variables=["question", "history"], template=question_template)
question_llm = ChatOpenAI(model_name=GPT_MODEL, temperature=0.7, openai_api_key=OPENAI_API_KEY)
question_chain = LLMChain(llm=question_llm, prompt=question_prompt_template, output_key="answer")
```

æ¬¡ã«è³ªå•äºˆæ¸¬ç”¨ LLM ã®ä½œæˆã§ã™ã€‚

prediction_template ã§å›ç­”çµæœã‚’ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã¨ã—ã¦ã‚»ãƒƒãƒˆã€è³ªå•äºˆæ¸¬ã®æŒ‡ç¤ºãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½œæˆã—ã¾ã™ã€‚

```py
### è³ªå•äºˆæ¸¬

# prediction_templateè¨³
# æ¬¡ã®3ã¤ã®è³ªå•ã‚’äºˆæ¸¬ã—ã¦ãã ã•ã„ã€‚ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒä¸ãˆã‚‰ã‚ŒãŸæ–‡è„ˆã«å¿œã˜ã¦ãã‚Œãã‚Œ20æ–‡å­—ä»¥å†…ã§è³ªå•ã—ã¾ã™ã€‚æ—¥æœ¬èªã§ç­”ãˆã¦ãã ã•ã„ã€‚
prediction_template = """Predict the next 3 questions the user will ask in response to the given context, each within 20 characters. Please answer in Japanese language.

### Context ###
{answer}
"""
prediction_prompt_template = PromptTemplate(input_variables=["answer"], template=prediction_template)
prediction_llm = ChatOpenAI(model_name=GPT_MODEL, temperature=0.7, openai_api_key=OPENAI_API_KEY)
prediction_chain = LLMChain(llm=prediction_llm, prompt=prediction_prompt_template, output_key="prediction")
```

æœ€å¾Œã« SequentialChain ã® memory ã«ä¼šè©±å±¥æ­´ã‚’è¨­å®šã€å›ç­”ç”¨ LLM -> è³ªå•äºˆæ¸¬ç”¨ LLM ã®é †ã«è¨­å®šã—å®Ÿè¡Œã—ã¾ã™ã€‚

```py
from langchain.chains import SequentialChain

### æ¨è«–å®Ÿè¡Œ
question_answer_prediction_chain = SequentialChain(
    memory=memory,
    chains=[question_chain, prediction_chain],
    input_variables=["question"],
    output_variables=["answer", "prediction"],
    verbose=True)
result = question_answer_prediction_chain({"question": "å€¤æ®µã¯ï¼Ÿ"})
result
```

ä»¥ä¸‹æ¨è«–çµæœã¨ãªã‚Šã¾ã™ã€‚

```
> Entering new SequentialChain chain...

> Finished chain.
{'question': 'å€¤æ®µã¯ï¼Ÿ',
 'history': 'Human: é£Ÿã¹ç‰©ã®ä¸­ã§ä½•ãŒå¥½ãï¼Ÿ\nAI: ç§ã¯é£Ÿã¹ç‰©ã‚’é£Ÿã¹ã‚‹ã“ã¨ã¯ã§ãã¾ã›ã‚“ãŒã€æ—¥æœ¬ã®æ–‡åŒ–ã«èˆˆå‘³ã‚’æŒã£ã¦ãŠã‚Šã€å¯¿å¸ã‚„ãƒ©ãƒ¼ãƒ¡ãƒ³ã€å¤©ã·ã‚‰ã€ç„¼ãé³¥ãªã©ã®æ—¥æœ¬æ–™ç†ãŒå¥½ãã§ã™ã€‚ã¾ãŸã€ã‚¤ãƒ³ãƒ‰ã‚«ãƒ¬ãƒ¼ã‚„ã‚¿ã‚¤æ–™ç†ã€ãƒ¡ã‚­ã‚·ã‚³æ–™ç†ãªã©ã€å¤šæ§˜ãªå›½ã®æ–™ç†ã‚‚å¥½ãã§ã™ã€‚\nHuman: ãã®ä¸­ã§ä½•ãŒå¥½ãï¼Ÿ\nAI: ç§ã¯ãŠã™ã™ã‚ã®æ–™ç†ã‚’é¸ã¶ã“ã¨ãŒã§ãã¾ã›ã‚“ãŒã€æ—¥æœ¬ã®å¯¿å¸ã¯éå¸¸ã«äººæ°—ãŒã‚ã‚Šã€æ–°é®®ãªé­šä»‹é¡ã‚’ä½¿ç”¨ã—ã¦ã„ã‚‹ã“ã¨ãŒå¤šã„ãŸã‚ã€ãŠã™ã™ã‚ã§ã™ã€‚ã¾ãŸã€ãƒ©ãƒ¼ãƒ¡ãƒ³ã¯æ—¥æœ¬ã®ã‚½ã‚¦ãƒ«ãƒ•ãƒ¼ãƒ‰ã®ä¸€ã¤ã§ã€æ§˜ã€…ãªç¨®é¡ãŒã‚ã‚Šã¾ã™ãŒã€è±šéª¨ãƒ©ãƒ¼ãƒ¡ãƒ³ã‚„é†¤æ²¹ãƒ©ãƒ¼ãƒ¡ãƒ³ãŒç‰¹ã«äººæ°—ãŒã‚ã‚Šã¾ã™ã€‚å¤©ã·ã‚‰ã‚‚ã€ã‚µã‚¯ã‚µã‚¯ã¨ã—ãŸé£Ÿæ„ŸãŒç¾å‘³ã—ã„ã§ã™ã€‚ç„¼ãé³¥ã¯ã€ä¸²ã«åˆºã—ãŸé¶è‚‰ã‚’ç‚­ç«ã§ç„¼ã„ãŸã‚‚ã®ã§ã€ãƒ“ãƒ¼ãƒ«ã¨ã®ç›¸æ€§ãŒè‰¯ã„ã§ã™ã€‚\n',
 'answer': 'ãã‚Œãã‚Œã®æ–™ç†ã«ã‚ˆã£ã¦å€¤æ®µã¯ç•°ãªã‚Šã¾ã™ãŒã€å¯¿å¸ã‚„ãƒ©ãƒ¼ãƒ¡ãƒ³ã€å¤©ã·ã‚‰ã€ç„¼ãé³¥ãªã©ã¯ã€æ—¥æœ¬ã®å±…é…’å±‹ã‚„ãƒ¬ã‚¹ãƒˆãƒ©ãƒ³ã§æ¯”è¼ƒçš„æ‰‹è»½ã«é£Ÿã¹ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚ã¾ãŸã€é«˜ç´šãªãŠåº—ã§ã¯ã€ã‚ˆã‚Šé«˜ä¾¡ãªæ–™ç†ã‚‚ã‚ã‚Šã¾ã™ã€‚ãŸã ã—ã€æ—¥æœ¬ã®é£Ÿæ–‡åŒ–ã§ã¯ã€å€¤æ®µãŒé«˜ã„ã‹ã‚‰ã¨ã„ã£ã¦å¿…ãšã—ã‚‚ç¾å‘³ã—ã„ã¨ã¯é™ã‚Šã¾ã›ã‚“ã€‚åœ°å…ƒã®äººãŒé€šã†ã‚ˆã†ãªã€å‘³ã®è‰¯ã„ãŠåº—ã‚’æ¢ã™ã®ãŒãŠã™ã™ã‚ã§ã™ã€‚ğŸ£ğŸœğŸ¢ğŸ—ğŸ’°',
 'prediction': '1. ã©ã®ãã‚‰ã„ã®å€¤æ®µã§ã™ã‹ï¼Ÿ\n2. ã©ã“ã§æ¢ã›ã°ã„ã„ã§ã™ã‹ï¼Ÿ\n3. ä½•ãŒãŠã™ã™ã‚ã§ã™ã‹ï¼Ÿ'}
```

å›ç­”çµæœãŒè¿”å´ã•ã‚Œã¾ã™ã€‚

```
'answer': 'ãã‚Œãã‚Œã®æ–™ç†ã«ã‚ˆã£ã¦å€¤æ®µã¯ç•°ãªã‚Šã¾ã™ãŒã€å¯¿å¸ã‚„ãƒ©ãƒ¼ãƒ¡ãƒ³ã€å¤©ã·ã‚‰ã€ç„¼ãé³¥ãªã©ã¯ã€æ—¥æœ¬ã®å±…é…’å±‹ã‚„ãƒ¬ã‚¹ãƒˆãƒ©ãƒ³ã§æ¯”è¼ƒçš„æ‰‹è»½ã«é£Ÿã¹ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚ã¾ãŸã€é«˜ç´šãªãŠåº—ã§ã¯ã€ã‚ˆã‚Šé«˜ä¾¡ãªæ–™ç†ã‚‚ã‚ã‚Šã¾ã™ã€‚ãŸã ã—ã€æ—¥æœ¬ã®é£Ÿæ–‡åŒ–ã§ã¯ã€å€¤æ®µãŒé«˜ã„ã‹ã‚‰ã¨ã„ã£ã¦å¿…ãšã—ã‚‚ç¾å‘³ã—ã„ã¨ã¯é™ã‚Šã¾ã›ã‚“ã€‚åœ°å…ƒã®äººãŒé€šã†ã‚ˆã†ãªã€å‘³ã®è‰¯ã„ãŠåº—ã‚’æ¢ã™ã®ãŒãŠã™ã™ã‚ã§ã™ã€‚ğŸ£ğŸœğŸ¢ğŸ—ğŸ’°'
```

è³ªå•äºˆæ¸¬ã‚‚è¿”å´ã•ã‚Œã‚‹ã®ã§ã“ã¡ã‚‰ã‚’ LINE ã®è³ªå•äºˆæ¸¬ãƒœã‚¿ãƒ³ã«ã‚»ãƒƒãƒˆã—ã¾ã™ã€‚

```
'prediction': '1. ã©ã®ãã‚‰ã„ã®å€¤æ®µã§ã™ã‹ï¼Ÿ\n2. ã©ã“ã§æ¢ã›ã°ã„ã„ã§ã™ã‹ï¼Ÿ\n3. ä½•ãŒãŠã™ã™ã‚ã§ã™ã‹ï¼Ÿ'
```

ä»¥ä¸‹ ãƒ—ãƒ­ãƒ€ã‚¯ãƒˆã«çµ„ã¿è¾¼ã‚€ Python ã‚³ãƒ¼ãƒ‰å…¨æ–‡ã¨ãªã‚Šã¾ã™ã€‚

:::details langchain_api.py

```py
import re
from typing import Dict, List, Tuple

import const
from langchain import LLMChain, PromptTemplate
from langchain.chains import SequentialChain
from langchain.chat_models import ChatOpenAI
from langchain.memory import SimpleMemory

GPT_MODEL = 'gpt-3.5-turbo'
TEMPERATURE = 0.7

# question_templateè¨³
# ä»¥ä¸‹ã¯ã€äººé–“ã¨AIã®å‹å¥½çš„ãªä¼šè©±ã§ã™ã€‚
# AIã¯ãŠã—ã‚ƒã¹ã‚Šã§ã€ãã®æ–‡è„ˆã‹ã‚‰ãŸãã•ã‚“ã®å…·ä½“çš„ãªè©³ç´°ã‚’æä¾›ã—ã¾ã™ã€‚
# ãã‚Œã‹ã‚‰ã€ãƒ•ã‚©ãƒ¼ãƒãƒ«ãªè¨€è‘‰ã‚’ä½¿ã†ã®ã‚’ã‚„ã‚ã¦ã€å‹é”ã®ã‚ˆã†ã«è¦ªã—ã¿ã‚„ã™ãè©±ã—ã¦ãã ã•ã„ã€‚ã¾ãŸã€ãŸãã•ã‚“ã®çµµæ–‡å­—ã‚’ä½¿ã£ã¦è©±ã—ã¦ãã ã•ã„ã€‚
# AIãŒè³ªå•ã«ç­”ãˆã‚‰ã‚Œãªã„å ´åˆã¯ã€æ­£ç›´ã«ã‚ã‹ã‚‰ãªã„ã¨è¨€ã„ã¾ã™ã€‚
# æ—¥æœ¬èªã§ãŠç­”ãˆãã ã•ã„ã€‚

QUESTION_TEMPLATE = """The following is a friendly conversation between a human and an AI.
The AI is talkative and provides lots of specific details from its context.
After that, stop using formal language. Talk to me in a friendly way, like a friend. Also, use lots of emojis when you talk.
If the AI does not know the answer to a question, it truthfully says it does not know.
Please answer in Japanese language.

Current conversation:
{history}
Human: {question}
AI:"""

# prediction_templateè¨³
# æ¬¡ã®3ã¤ã®è³ªå•ã‚’äºˆæ¸¬ã—ã¦ãã ã•ã„ã€‚ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒä¸ãˆã‚‰ã‚ŒãŸæ–‡è„ˆã«å¿œã˜ã¦ãã‚Œãã‚Œ20æ–‡å­—ä»¥å†…ã§è³ªå•ã—ã¾ã™ã€‚æ—¥æœ¬èªã§ç­”ãˆã¦ãã ã•ã„ã€‚
PREDICTION_TEMPLATE = """Predict the next 3 questions the user will ask in response to the given context, each within 20 characters. Please answer in Japanese language.

### Context ###
{answer}
"""


def create_simple_memory(history):
    memories = {"history": history}
    memory = SimpleMemory(memories=memories)
    return memory


def create_question_chain():
    question_prompt_template = PromptTemplate(input_variables=["question", "history"], template=QUESTION_TEMPLATE)
    question_llm = ChatOpenAI(model_name=GPT_MODEL, temperature=TEMPERATURE, openai_api_key=const.OPEN_AI_API_KEY)
    question_chain = LLMChain(llm=question_llm, prompt=question_prompt_template, output_key="answer")
    return question_chain


def create_prediction_chain():
    prediction_prompt_template = PromptTemplate(input_variables=["answer"], template=PREDICTION_TEMPLATE)
    prediction_llm = ChatOpenAI(model_name=GPT_MODEL, temperature=TEMPERATURE, openai_api_key=const.OPEN_AI_API_KEY)
    prediction_chain = LLMChain(llm=prediction_llm, prompt=prediction_prompt_template, output_key="prediction")
    return prediction_chain


def create_question_answer_prediction_chain(memory, question_chain, prediction_chain):
    question_answer_prediction_chain = SequentialChain(
        memory=memory,
        chains=[question_chain, prediction_chain],
        input_variables=["question"],
        output_variables=["answer", "prediction"],
        verbose=True)
    return question_answer_prediction_chain


def parse_history_prompts(messages: List[Dict[str, str]]) -> Tuple[str, str]:
    formatted_messages = []
    question = ""

    for i, message in enumerate(messages):
        if message["role"] == "user":
            role = "Human"
        elif message["role"] == "assistant":
            role = "AI"
        formatted_messages.append(f"{role}: {message['content']}")

        # å…¥åŠ›é…åˆ—ã®æœ€å¾Œã®è¡Œã®contentã‚’questionå¤‰æ•°ã«æ ¼ç´
        if i == len(messages) - 1 and message["role"] == "user":
            question = message["content"]

    # æ–‡å­—åˆ—ã«çµåˆ
    history = "\n".join(formatted_messages)
    return history, question


def _extract_prediction_and_answer(data: Dict[str, str]) -> Tuple[List[str], str]:
    prediction = data["prediction"]
    assistant_answer = data["answer"]

    # æ”¹è¡Œã§åˆ†å‰²ã—ã€æ–‡å­—åˆ—ã®å‰å¾Œã®ç©ºç™½ã¨æ”¹è¡Œã‚’å‰Šé™¤
    predictions = list(map(lambda line: _remove_ordinal_number(line.strip()), prediction.strip().split('\n')))
    return assistant_answer, predictions


# æ–‡å­—åˆ—ã®å…ˆé ­ã«ã‚ã‚‹åºæ•°ã‚’å‰Šé™¤ã™ã‚‹é–¢æ•°
def _remove_ordinal_number(text: str) -> str:
    # æ­£è¦è¡¨ç¾ã§å…ˆé ­ã®æ•°å­—ã¨ãƒ”ãƒªã‚ªãƒ‰ã‚’å‰Šé™¤
    return re.sub(r'^\d+\.\s*', '', text)


def completions(history_prompts: List[Dict[str, str]]) -> Tuple[str, str, List[str]]:
    history, question = parse_history_prompts(history_prompts)
    memory = create_simple_memory(history)
    question_chain = create_question_chain()
    prediction_chain = create_prediction_chain()
    question_answer_prediction_chain = create_question_answer_prediction_chain(memory, question_chain, prediction_chain)
    result = question_answer_prediction_chain({"question": question})
    assistant_answer, predictions = _extract_prediction_and_answer(result)
    return assistant_answer, predictions
```

:::

### Pros

å›ç­” LLM å®Ÿè¡Œå¾Œã€å›ç­”çµæœã‹ã‚‰è³ªå•äºˆæ¸¬ç”¨ LLM ã«æ¨è«–ã•ã›ã‚‹ã®ã§ç¢ºå®Ÿã«è³ªå•äºˆæ¸¬ã‚’å–å¾—ã™ã‚‹äº‹ãŒã§ãã¾ã™ã€‚

### Cons

è³ªå•ã«å¯¾ã™ã‚‹æ¨è«–ã¨ã€è³ªå•äºˆæ¸¬ã®æ¨è«– 2 å› ChatGPT API ã«ãƒªã‚¯ã‚¨ã‚¹ãƒˆãŒæŠ•ã’ã‚‰ã‚Œã‚‹ã®ã§ã€ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãŒé…ã„ã§ã™ã€‚

å®Ÿéš›ã« GPT-4 API ã§å®Ÿè¡Œã—ãŸæ‰€ Timeout ã‚¨ãƒ©ãƒ¼ã¨ãªã‚‹äº‹ãŒã‚ã‚Šã¾ã—ãŸã€‚

# LINE ã®ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³ã«è³ªå•äºˆæ¸¬ãƒœã‚¿ãƒ³ã‚’è¨­å®šã™ã‚‹

LINE ã®ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³ã«è³ªå•äºˆæ¸¬ãƒœã‚¿ãƒ³ã‚’è¨­ç½®ã™ã‚‹ã«ã¯ LINE SDK ã® QuickReplyButton ã‚’ä½¿ã„ã¾ã™ã€‚

QuickReplyButton ã«è³ªå•äºˆæ¸¬ã‚’è¨­å®šã™ã‚‹ã®ã§ã™ãŒã€æœ€å¤§æ–‡å­—æ•° 20 æ–‡å­—ãªã®ã§ã‚»ãƒƒãƒˆã™ã‚‹å‰ã«æ–‡å­—æ•°ãƒã‚§ãƒƒã‚¯ã‚’ã—ã¾ã™ã€‚

QuickReply ãƒ–ã‚¸ã‚§ã‚¯ãƒˆã¨æ¨è«–çµæœã‚’ TextSendMessage ã«è¿½åŠ ã—ã¦ LineBotApi ã® reply_message ã§ LINE ã‚µãƒ¼ãƒã«è¿”å´ã™ã‚Œã°è³ªå•äºˆæ¸¬ãƒœã‚¿ãƒ³ãŒè¡¨ç¤ºã•ã‚Œã¾ã™ã€‚

```py
from typing import List

import const
from linebot import LineBotApi
from linebot.models import (MessageAction, QuickReply, QuickReplyButton,
                            TextSendMessage)


def reply_message_for_line(reply_token: str, assistant_answer: str, predictions: List[str]):
    try:
        # Create an instance of the LineBotApi with the Line channel access token
        line_bot_api = LineBotApi(const.LINE_CHANNEL_ACCESS_TOKEN)
        # predications é…åˆ—å†…ã®è¦ç´ ã®æ–‡å­—åˆ—ãŒ21æ–‡å­—ä»¥ä¸Šã®å ´åˆé…åˆ—ã‹ã‚‰å‰Šé™¤
        predictions = list(filter(lambda line: len(line) <= 20 and line != "", predictions))
        # predications ãŒç©ºã ã£ãŸã‚‰ message ã« TextSendMessage ã‚’è¨­å®š
        if len(predictions) == 0:
            message = TextSendMessage(text=assistant_answer)
        else:
            # ã‚¯ã‚¤ãƒƒã‚¯ãƒªãƒ—ãƒ©ã‚¤ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’ä½œæˆ
            quick_reply_actions = map(lambda line: QuickReplyButton(action=MessageAction(label=line, text=line)), predictions)
            # ã‚¯ã‚¤ãƒƒã‚¯ãƒªãƒ—ãƒ©ã‚¤ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œæˆ
            quick_reply = QuickReply(items=quick_reply_actions)
            # ãƒ†ã‚­ã‚¹ãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã«ã‚¯ã‚¤ãƒƒã‚¯ãƒªãƒ—ãƒ©ã‚¤ã‚’è¿½åŠ 
            message = TextSendMessage(text=assistant_answer, quick_reply=quick_reply)

        # Reply the message using the LineBotApi instance
        line_bot_api.reply_message(reply_token, message)

    except Exception as e:
        raise e
```

ãã®ä»–ã€DynamoDB æ“ä½œç­‰ã®ã‚³ãƒ¼ãƒ‰ã¯ [ã“ã¡ã‚‰](https://zenn.dev/zuma_lab/articles/gpt-4-line-chatbot#%E5%AE%9F%E8%A3%85%E3%82%B3%E3%83%BC%E3%83%89) ã«æ²è¼‰ã—ã¦ãŠã‚Šã¾ã™ã€‚

å‚è€ƒã«ãªã‚Œã°å¹¸ã„ã§ã™ã€‚
